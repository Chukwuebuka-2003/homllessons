


from pathlib import Path
import pandas as pd
import tarfile 
import urllib.request
import matplotlib.pyplot as plt


%config IPCompleter.greedy=True


housing = pd.read_csv("housing.csv")


housing.info()


housing.isna().sum()


housing['ocean_proximity'].value_counts()


housing.describe()


import numpy as np


housing["income_cat"] = pd.cut(housing["median_income"],
bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
labels=[1, 2, 3, 4, 5])


housing["income_cat"].value_counts().sort_index().plot.bar(rot=0, grid=True)
plt.xlabel("Income category")
plt.ylabel("Number of districts")
plt.show()


from sklearn.model_selection import StratifiedShuffleSplit
splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []

for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])


strat_train_set, strat_test_set = strat_splits[0]


strat_test_set


from sklearn.model_selection import train_test_split


strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, stratify=housing["income_cat"], random_state=42)


strat_test_set["income_cat"].value_counts() / len(strat_test_set)


for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)


housing = strat_train_set.copy()


import seaborn as sns

sns.relplot(x="longitude", y="latitude",
           data=housing, kind="scatter", hue="ocean_proximity", alpha=0.2)

plt.show()


housing.plot(kind="scatter", x="longitude", y="latitude", grid=True,
s=housing["population"] / 100, label="population",
c="median_house_value", cmap="jet", colorbar=True,
legend=True, sharex=False)
plt.show()


corr_matrix = housing.drop("ocean_proximity", axis=1).corr()
corr_matrix


sns.relplot(x="median_income", y="median_house_value",
           data=housing, kind="scatter")


housing.isna().sum()


#housing.drop("total_bedrooms", axis=1)


from sklearn.impute import SimpleImputer


housing_num  = housing.select_dtypes(include=[np.number])
housing_num


imputer = SimpleImputer(strategy="median")

imputer.fit(housing_num)


X = imputer.transform(housing_num)
X


housing.isna().sum()


numeric_cols = housing.select_dtypes(include=['number']).columns
numeric_cols


# Impute missing values for numeric columns
numeric_imputer = SimpleImputer(strategy='median')

housing[numeric_cols] = numeric_imputer.fit_transform(housing[numeric_cols])
housing_num = housing[numeric_cols]


housing_num


housing.isna().sum()





housing.info()


#median = housing["total_bedrooms"].median() # option 3
#housing["total_bedrooms"].fillna(median, inplace=True)


housing.isna().sum()


categorical_cols = housing.select_dtypes(include=['object']).columns
categorical_cols


housing_cat  = housing[['ocean_proximity']]
housing_cat





from sklearn.preprocessing import OrdinalEncoder


ordinal_encoder = OrdinalEncoder()
housing_encode = ordinal_encoder.fit_transform(housing_cat)


housing_encode








from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder()
housing_onehot = onehot_encoder.fit_transform(housing_cat)
housing_onehot





housing_onehot.toarray()











from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)


housing_num_min_max_scaled





from sklearn.preprocessing import StandardScaler
std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_num)


from sklearn.preprocessing import StandardScaler


housing_num_std_scaled





from sklearn.linear_model import LinearRegression


target = StandardScaler()


housing = strat_train_set.drop("median_house_value", axis=1)
housing_labels = strat_train_set["median_house_value"].copy()


scaled_labels = target.fit_transform(housing_labels.to_frame())


model = LinearRegression()
model


model.fit(housing[["median_income"]], scaled_labels)


some_new_data = housing[["median_income"]].iloc[:5] 
some_new_data


scaled_predictions = model.predict(some_new_data)
predictions = target.inverse_transform(scaled_predictions)


predictions








from sklearn.compose import TransformedTargetRegressor


model = TransformedTargetRegressor(
    LinearRegression(), 
    transformer = StandardScaler()
)


model.fit(housing[["median_income"]], housing_labels)


predictions = model.predict(some_new_data)


predictions








from sklearn.preprocessing import FunctionTransformer


log_transform = FunctionTransformer(np.log, inverse_func= np.exp)


log_transform


log_pop = log_transform.fit_transform(housing[['population']])
log_pop











# building a pipeline
from sklearn.pipeline import Pipeline
#import sklearn
#sklearn.set_config(display="diagram")


from sklearn.preprocessing import StandardScaler
numerical_pipeline = Pipeline([(
    "imputer", SimpleImputer(strategy="median")),
                              ("standardize", StandardScaler())])


numerical_pipeline





from sklearn.pipeline import make_pipeline
num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())


housing_num_prepared = num_pipeline.fit_transform(housing_num)
housing_num_prepared[:2].round(2)


housing_numpipeline = numerical_pipeline.fit_transform(housing_num)
housing_numpipeline


df_housing_num = pd.DataFrame(
housing_numpipeline, columns=numerical_pipeline.get_feature_names_out(),
index=housing_num.index)


df_housing_num








df2 = housing
df2.head()


# building a columntransformer pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import make_pipeline
numerical_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',
       'total_bedrooms', 'population', 'households', 'median_income']

categorical_features = ['ocean_proximity']

numerical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median"))
])


categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
    
])

preprocessing = ColumnTransformer([
    ("numerical", numerical_pipeline, numerical_features),
    ("categorical", categorical_pipeline, categorical_features)
])


preprocessing


from sklearn.compose import make_column_selector, make_column_transformer

preprocessed = make_column_transformer(
    (numerical_pipeline, make_column_selector(dtype_include=np.number)),
    (categorical_pipeline, make_column_selector(dtype_include=object)),

)
preprocessed


housing_col_prepared = preprocessed.fit_transform(housing)


from sklearn.pipeline import make_pipeline

num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

housing_num_prepared = num_pipeline.fit_transform(housing_num)

housing_num_prepared[:2].round(2)


df_housing_num_prepared = pd.DataFrame(
housing_num_prepared, columns=num_pipeline.get_feature_names_out(),
index=housing_num.index)

df_housing_num_prepared.head()


from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np


# Identify numeric and categorical features
#num_feature = df2.select_dtypes(include=["number"]).columns.tolist()
#cat_feature = df2.select_dtypes(include=["object"]).columns.tolist()
num_feature = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']
cat_feature = ['ocean_proximity']

# Numeric pipeline
numeric_imputer = SimpleImputer(strategy='median')
num_pipeline = Pipeline([
    ("imputer", numeric_imputer),
    ("standardize", StandardScaler())
])

# Apply numeric imputation to the DataFrame
df2[num_feature] = numeric_imputer.fit_transform(df2[num_feature])
df_num = df2[num_feature]

# Categorical pipeline
cat_imputer = SimpleImputer(strategy="most_frequent")
cat_pipeline = Pipeline([
    ("imputer", cat_imputer),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# Apply categorical imputation to the DataFrame
df2[cat_feature] = cat_imputer.fit_transform(df2[cat_feature])
df_cat = df2[cat_feature]

# Combine the pipelines into a single ColumnTransformer
prep = ColumnTransformer([
    ("numerical", num_pipeline, num_feature),
    ("categorical", cat_pipeline, cat_feature)
])



housing_columnpreprocessed = prep.fit_transform(df2)
housing_columnpreprocessed.shape


prep.get_feature_names_out()


housing.isna().sum()





from sklearn.compose import make_column_selector, make_column_transformer

preprocess = make_column_transformer(
(numerical_pipeline, make_column_selector(dtype_include=np.number)),
(categorical_pipeline, make_column_selector(dtype_include=object)),
)

preprocess


housing


preprocess


housing_prepared = preprocess.fit_transform(housing)
housing_prepared


from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted


from sklearn.cluster import KMeans

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state
    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, random_state= self.random_state)
        self.kmeans_.fit(X, sample_weight = sample_weight)
        return self
    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    def get_feature_names_out(self,names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
    


from sklearn.metrics.pairwise import rbf_kernel

age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)

age_simil_35


cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)

similarities = cluster_simil.fit_transform(housing[["latitude", "longitude"]],sample_weight=housing_labels)

similarities[:3].round(2)


def column_ratio(X):
    return X[:, [0]] / X[:, [1]]

def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())
log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler())

cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1, random_state=42)
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler())

preprocessing = ColumnTransformer([
    ("bedrooms", ratio_pipeline(),["total_bedrooms", "total_rooms"]),
    ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]),
    ("people_per_house", ratio_pipeline(), ["population", "households"]),
    ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population","households", "median_income"]),
    ("geo", cluster_simil,["latitude", "longitude"]),
    ("cat", cat_pipeline, make_column_selector(dtype_include=object)),
],
remainder = default_num_pipeline)


housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape


preprocessing.get_feature_names_out()








from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)


housing_predictions = lin_reg.predict(housing)
housing_predictions[:5].round(-2)


# measuring the regression model RMSE with sklearn mean_squared_error
from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(housing_labels, housing_predictions,
squared=False)

lin_rmse














from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))

tree_reg.fit(housing, housing_labels)


housing_predictions = tree_reg.predict(housing)
tree_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)

tree_rmse








import warnings
warnings.filterwarnings("ignore")


from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)

pd.Series(tree_rmses).describe()








from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))

forest_reg.fit(housing, housing_labels)


forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,
scoring="neg_root_mean_squared_error", cv=10)


pd.Series(forest_rmses).describe()


housing_predictions = forest_reg.predict(housing)


forestreg_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)
forestreg_rmse














from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])

param_grid =  [
    {'preprocessing__geo__n_clusters':[5,8,10],
    'random_forest__max_features':[4,6,8]},
    {'preprocessing__geo__n_clusters':[10,15],
    'random_forest__max_features':[6,8,10]},
]

grid_search = GridSearchCV(full_pipeline, param_grid, cv=3, scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)





grid_search.best_params_





x = grid_search.cv_results_
cv_result = pd.DataFrame(x)
cv_result.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_result.head()








from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs  = {'preprocessing__geo__n_clusters':randint(low=3, high=50),
                   'random_forest__max_features': randint(low=2, high=20)}


rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42
)

rnd_search


rnd_search.fit(housing, housing_labels)


#grid search took 7m37secs and RandomizedSearch took 6m9.8secs 











#randomforestregressor

final_model = rnd_search.best_estimator_
feature_importances = final_model["random_forest"].feature_importances_
feature_importances.round(2)


# sort the importance scores and display them next to their corresponding attribute names
#sorted(zip(feature_importances,
           #final_model["preprocessing"].get_feature_names_out()),
           #reverse=True)

x = sorted(zip(feature_importances, final_model["preprocessing"].get_feature_names_out()), reverse=True)
final_model_result = pd.DataFrame(x)
#cv_result.sort_values(by="mean_test_score", ascending=False, inplace=True)
final_model_result.head()








X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
final_rmse


# compute and test a confidence interval
from scipy import stats
confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors)-1,
                         loc = squared_errors.mean(),
                         scale=stats.sem(squared_errors)))





# save the model
import joblib
joblib.dump(final_model, "california_housing_model.sav")


# load and test the model
import joblib
#import KMeans, BaseEstimator, TransformerMixin, rbf_kernel

def column_ratio(X):
    def ratio_name(function_transformer, feature_names_in):
        class ClusterSimilarity(BaseEstimator, TransformerMixin):
            final_model_reloaded = joblib.load("california_housing_model.sav")


new_data 
